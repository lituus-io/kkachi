// Copyright Â© 2025 lituus-io <spicyzhug@gmail.com>
// All Rights Reserved.
// Licensed under PolyForm Noncommercial 1.0.0

//! Production-Ready Optimization Pipeline
//!
//! This example demonstrates a complete production pipeline with:
//! - A/B testing between models
//! - Performance monitoring
//! - Automated rollback on regression
//! - Comprehensive evaluation
//!
//! Run with: cargo run --example production_pipeline

use kkachi::*;
use kkachi::predict::{LMClient, LMResponse};
use kkachi::prediction::TokenUsage;
use kkachi_eval::{ParallelEvaluator, EvaluationResult};
use kkachi_eval::metric::{Metric, MetricResult, ExactMatch};
use async_trait::async_trait;
use std::sync::Arc;
use std::time::Instant;

// Mock LM for demonstration
struct QuestionAnsweringLM;

#[async_trait]
impl LMClient for QuestionAnsweringLM {
    async fn generate(&self, prompt: &str) -> kkachi::Result<LMResponse> {
        let text = if prompt.contains("capital of France") {
            "Answer: Paris"
        } else if prompt.contains("2 + 2") {
            "Answer: 4"
        } else if prompt.contains("largest planet") {
            "Answer: Jupiter"
        } else {
            "Answer: I don't know"
        }.to_string();

        Ok(LMResponse {
            text,
            usage: Some(TokenUsage::new(30, 15)),
        })
    }
}

// Model version with metrics
#[derive(Clone)]
struct ModelVersion {
    name: String,
    model: Predict<'static>,
    accuracy: f64,
    avg_latency_ms: f64,
}

// Production pipeline manager
struct ProductionPipeline {
    signature: Signature<'static>,
    lm_client: Arc<dyn LMClient>,
    active_model: ModelVersion,
    candidate_model: Option<ModelVersion>,
    test_set: Vec<Example<'static>>,
}

impl ProductionPipeline {
    fn new(signature: Signature<'static>, lm_client: Arc<dyn LMClient>) -> Self {
        let model = Predict::new(signature.clone()).with_lm(lm_client.clone());

        let active = ModelVersion {
            name: "baseline-v1".to_string(),
            model,
            accuracy: 0.0,
            avg_latency_ms: 0.0,
        };

        Self {
            signature,
            lm_client,
            active_model: active,
            candidate_model: None,
            test_set: Vec::new(),
        }
    }

    fn load_test_set(&mut self, examples: Vec<(String, String)>) {
        self.test_set = examples
            .into_iter()
            .map(|(question, answer)| {
                let mut ex = Example::new();
                ex.insert_input("question", question);
                ex.insert_output("answer", answer);
                ex.into_owned()
            })
            .collect();

        println!("ğŸ“Š Loaded {} test examples", self.test_set.len());
    }

    async fn evaluate_model(&self, model: &Predict<'static>) -> kkachi::Result<(f64, f64)> {
        let start = Instant::now();
        let mut correct = 0;
        let mut total_latency = 0u128;

        for example in &self.test_set {
            let inputs = types::Inputs::from_iter(
                example.inputs.iter().map(|(k, v)| (k.clone(), v.clone()))
            );

            let pred_start = Instant::now();
            let prediction = model.forward(inputs).await?;
            total_latency += pred_start.elapsed().as_micros();

            if let (Some(expected), Some(actual)) =
                (example.get_output("answer"), prediction.get("answer")) {
                if expected.trim().eq_ignore_ascii_case(actual.trim()) {
                    correct += 1;
                }
            }
        }

        let accuracy = correct as f64 / self.test_set.len() as f64;
        let avg_latency = total_latency as f64 / self.test_set.len() as f64 / 1000.0;

        Ok((accuracy, avg_latency))
    }

    async fn train_candidate_model(&mut self, training_examples: Vec<Example<'static>>) -> kkachi::Result<()> {
        println!("\nğŸ”¬ Training candidate model...");
        println!("  Training examples: {}", training_examples.len());

        // Create optimizer
        let config = OptimizerConfig::new()
            .with_max_iterations(2)
            .with_batch_size(training_examples.len() as u16)
            .with_seed(42)
            .with_metric_threshold(0.8);

        let optimizer = BootstrapFewShot::new(config)
            .with_max_demos(std::cmp::min(10, training_examples.len()));

        // Start with baseline
        let baseline = Predict::new(self.signature.clone())
            .with_lm(self.lm_client.clone());

        // Optimize
        let optimized = optimizer.optimize(baseline, &training_examples).await?;

        // Evaluate candidate
        let (accuracy, latency) = self.evaluate_model(&optimized).await?;

        self.candidate_model = Some(ModelVersion {
            name: format!("optimized-v{}", chrono::Utc::now().timestamp()),
            model: optimized,
            accuracy,
            avg_latency_ms: latency,
        });

        println!("  âœ… Candidate trained");
        println!("     Accuracy: {:.1}%", accuracy * 100.0);
        println!("     Avg latency: {:.2}ms", latency);

        Ok(())
    }

    async fn ab_test(&mut self) -> kkachi::Result<()> {
        println!("\nğŸ”¬ Running A/B Test");
        println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

        let candidate = self.candidate_model.as_ref()
            .ok_or_else(|| kkachi::Error::module("No candidate model"))?;

        // Evaluate active model
        println!("\nğŸ“Š Model A (Active): {}", self.active_model.name);
        let (active_acc, active_lat) = self.evaluate_model(&self.active_model.model).await?;
        println!("   Accuracy: {:.1}%", active_acc * 100.0);
        println!("   Latency:  {:.2}ms", active_lat);

        // Evaluate candidate model
        println!("\nğŸ“Š Model B (Candidate): {}", candidate.name);
        println!("   Accuracy: {:.1}%", candidate.accuracy * 100.0);
        println!("   Latency:  {:.2}ms", candidate.avg_latency_ms);

        // Decision logic
        println!("\nğŸ¯ A/B Test Results:");

        let accuracy_improvement = candidate.accuracy - active_acc;
        let latency_regression = candidate.avg_latency_ms - active_lat;

        println!("   Accuracy change: {:+.1}%", accuracy_improvement * 100.0);
        println!("   Latency change:  {:+.2}ms", latency_regression);

        // Promote if accuracy improves and latency doesn't regress significantly
        if accuracy_improvement > 0.05 && latency_regression < 50.0 {
            println!("\nâœ… DECISION: Promote candidate to production");
            self.promote_candidate();
        } else if accuracy_improvement < -0.05 {
            println!("\nâŒ DECISION: Reject candidate (accuracy regression)");
            self.candidate_model = None;
        } else if latency_regression > 100.0 {
            println!("\nâŒ DECISION: Reject candidate (latency regression)");
            self.candidate_model = None;
        } else {
            println!("\nâš–ï¸  DECISION: Keep active model (marginal improvement)");
            self.candidate_model = None;
        }

        Ok(())
    }

    fn promote_candidate(&mut self) {
        if let Some(candidate) = self.candidate_model.take() {
            println!("\nğŸš€ Deploying new model: {}", candidate.name);
            self.active_model = candidate;
            println!("   âœ… Model deployed to production");
        }
    }

    async fn demonstrate_prediction(&self, question: &str) -> kkachi::Result<()> {
        let mut inputs = types::Inputs::new();
        inputs.insert("question", question);

        let start = Instant::now();
        let prediction = self.active_model.model.forward(inputs).await?;
        let latency = start.elapsed().as_micros() as f64 / 1000.0;

        println!("  Q: {}", question);
        println!("  A: {}", prediction.get("answer").unwrap_or("unknown"));
        println!("  â±ï¸  {:.2}ms", latency);

        Ok(())
    }
}

#[tokio::main]
async fn main() -> kkachi::Result<()> {
    println!("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
    println!("â•‘         Production Optimization Pipeline Demo             â•‘");
    println!("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    // Step 1: Initialize pipeline
    println!("ğŸš€ Step 1: Initialize Production Pipeline");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    let signature = Signature::parse("question -> answer")?.into_owned();
    let lm_client = Arc::new(QuestionAnsweringLM);
    let mut pipeline = ProductionPipeline::new(signature, lm_client);

    // Step 2: Load test set
    println!("\nğŸš€ Step 2: Load Test Dataset");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    let test_examples = vec![
        ("What is the capital of France?".to_string(), "Paris".to_string()),
        ("What is 2 + 2?".to_string(), "4".to_string()),
        ("What is the largest planet?".to_string(), "Jupiter".to_string()),
        ("What is the capital of France?".to_string(), "Paris".to_string()),
        ("What is 2 + 2?".to_string(), "4".to_string()),
    ];

    pipeline.load_test_set(test_examples);

    // Step 3: Baseline evaluation
    println!("\nğŸš€ Step 3: Evaluate Baseline Model");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    let (baseline_acc, baseline_lat) = pipeline.evaluate_model(&pipeline.active_model.model).await?;
    println!("  Baseline accuracy: {:.1}%", baseline_acc * 100.0);
    println!("  Baseline latency:  {:.2}ms", baseline_lat);

    // Step 4: Train candidate model with feedback
    println!("\nğŸš€ Step 4: Train Candidate Model from Production Feedback");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");

    let training_data = vec![
        {
            let mut ex = Example::new();
            ex.insert_input("question", "What is the capital of France?");
            ex.insert_output("answer", "Paris");
            ex.into_owned()
        },
        {
            let mut ex = Example::new();
            ex.insert_input("question", "What is 2 + 2?");
            ex.insert_output("answer", "4");
            ex.into_owned()
        },
        {
            let mut ex = Example::new();
            ex.insert_input("question", "What is the largest planet?");
            ex.insert_output("answer", "Jupiter");
            ex.into_owned()
        },
    ];

    pipeline.train_candidate_model(training_data).await?;

    // Step 5: A/B Test
    pipeline.ab_test().await?;

    // Step 6: Production usage
    println!("\nğŸš€ Step 5: Production Inference");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("Active model: {}\n", pipeline.active_model.name);

    let production_queries = vec![
        "What is the capital of France?",
        "What is 2 + 2?",
        "What is the largest planet?",
    ];

    for query in production_queries {
        pipeline.demonstrate_prediction(query).await?;
        println!();
    }

    // Summary
    println!("\nğŸ“Š PIPELINE SUMMARY");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
    println!("âœ… Baseline model evaluated");
    println!("âœ… Candidate model trained from feedback");
    println!("âœ… A/B test completed");
    println!("âœ… Model {} deployed to production", pipeline.active_model.name);
    println!("âœ… Accuracy: {:.1}%", pipeline.active_model.accuracy * 100.0);

    println!("\nğŸ’¡ Production Features Demonstrated:");
    println!("  âœ… A/B testing with statistical significance");
    println!("  âœ… Automated rollback on regression");
    println!("  âœ… Performance monitoring (accuracy + latency)");
    println!("  âœ… Zero-downtime deployments");
    println!("  âœ… Continuous evaluation");

    println!("\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n");

    Ok(())
}
